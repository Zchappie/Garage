{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment 10: Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exporting the results to PDF\n",
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook.\n",
    "2. Download the notebook in HTML (click File > Download as > .html)\n",
    "3. Convert the HTML to PDF using e.g. https://www.sejda.com/html-to-pdf or `wkhtmltopdf` for Linux ([tutorial](https://www.cyberciti.biz/open-source/html-to-pdf-freeware-linux-osx-windows-software/))\n",
    "4. Concatenate your solutions for other tasks with the output of Step 3. On a Linux machine you can simply use `pdfunite`, there are similar tools for other platforms too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "This way is preferred to using `nbconvert`, since `nbconvert` clips lines that exceed page width and makes your code harder to grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant recommendation\n",
    "\n",
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not yet rated based on a latent factor model.\n",
    "\n",
    "Specifically, the objective function (loss) we wanted to optimize is:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(i, x) \\in W} (M_{ix} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(i, x)$ pairs for which the rating $M_{ix}$ given by user $i$ to restaurant $x$ is known. Here we have also introduced two regularization terms to help us with overfitting where $\\lambda$ is hyper-parameter that control the strength of the regularization.\n",
    "\n",
    "**Hint 1**: Using the closed form solution for regression might lead to singular values. To avoid this issue perform the regression step with an existing package such as scikit-learn. It is advisable to use ridge regression to account for regularization.\n",
    "\n",
    "**Hint 2**: If you are using the scikit-learn package remember to set ``fit intercept = False`` to only learn the coeficients of the linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Data (nothing to do here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset. We store the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = np.max(ratings[:,0] + 1)\n",
    "n_restaurants = np.max(ratings[:,1] + 1)\n",
    "M = sp.coo_matrix((ratings[:,2], (ratings[:,0], ratings[:,1])), shape=(n_users, n_restaurants)).tocsr()\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the <a href=\"https://en.wikipedia.org/wiki/Cold_start_(computing)\"> cold start problem</a>, in the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings.\n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.\n",
    "\n",
    "**Note**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. We store the indices for which we the rating data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    shape = (-1, -1)\n",
    "    while matrix.shape != shape:\n",
    "        shape = matrix.shape\n",
    "        nnz = matrix>0\n",
    "        row_ixs = nnz.sum(1).A1 > min_entries\n",
    "        matrix = matrix[row_ixs]\n",
    "        nnz = matrix>0\n",
    "        col_ixs = nnz.sum(0).A1 > min_entries\n",
    "        matrix = matrix[:,col_ixs]\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement a function that substracts the mean user rating from the sparse rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "      \n",
    "    # YOUR CODE HERE\n",
    "    user_means = np.mean(matrix, axis=1)\n",
    "    user_means = user_means.reshape(-1, 1) # reshape to [N,1]\n",
    "    \n",
    "    matrix = matrix - user_means\n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train, validation and test set (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix_cp = matrix.copy()\n",
    "    non_zero_idx = np.argwhere(matrix_cp)\n",
    "    ixs = np.random.permutation(non_zero_idx)\n",
    "    val_idx = tuple(ixs[:n_validation].T)\n",
    "    test_idx = tuple(ixs[n_validation:n_validation + n_test].T)\n",
    "    \n",
    "    val_values = matrix_cp[val_idx].A1\n",
    "    test_values = matrix_cp[test_idx].A1\n",
    "    \n",
    "    matrix_cp[val_idx] = matrix_cp[test_idx] = 0\n",
    "    matrix_cp.eliminate_zeros()\n",
    "\n",
    "    return matrix_cp, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (3529, 2072)\n"
     ]
    }
   ],
   "source": [
    "M = cold_start_preprocessing(M, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove user means.\n",
    "nonzero_indices = np.argwhere(M_train)\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values - user_means[np.array(val_idx).T[:,0]].A1\n",
    "test_values_shifted = test_values - user_means[np.array(test_idx).T[:,0]].A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529, 2072)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3529, 2072)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(M_train.shape)\n",
    "M_shifted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the loss function (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(values, ixs, Q, P, reg_lambda):\n",
    "    \"\"\"\n",
    "    Compute the loss of the latent factor model (at indices ixs).\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : np.array, shape [n_ixs,]\n",
    "        The array with the ground-truth values.\n",
    "    ixs : tuple, shape [2, n_ixs]\n",
    "        The indices at which we want to evaluate the loss (usually the nonzero indices of the unshifted data matrix).\n",
    "    Q : np.array, shape [N, k]\n",
    "        The matrix Q of a latent factor model.\n",
    "    P : np.array, shape [k, D]\n",
    "        The matrix P of a latent factor model.\n",
    "    reg_lambda : float\n",
    "        The regularization strength\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "           The loss of the latent factor model.\n",
    "\n",
    "    \"\"\"\n",
    "    mean_sse_loss = np.sum((values - Q.dot(P)[ixs])**2)\n",
    "    regularization_loss =  reg_lambda * (np.sum(np.linalg.norm(P, axis=0)**2) + np.sum(np.linalg.norm(Q, axis=1) ** 2))\n",
    "    \n",
    "    return mean_sse_loss + regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating optimization\n",
    "\n",
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement a function that initializes the latent factors $Q$ and $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, 'random' means we initialize\n",
    "             the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    N = matrix.shape[0]\n",
    "    D = matrix.shape[1]\n",
    "    if init == 'random':\n",
    "        Q = np.random.rand(N, k)/k\n",
    "        P = np.random.rand(k, D)/k\n",
    "    elif init == 'svd':\n",
    "        U, S, V = np.linalg.svd(matrix, full_matrices=False)\n",
    "        Q = U*S\n",
    "        P = V\n",
    "    \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implement the alternating optimization approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=5, eval_every=1):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 5\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    # YOUR CODE HERE\n",
    "    # initialize the params\n",
    "    Q, P = initialize_Q_P(M, k, init)\n",
    "    best_Q = Q\n",
    "    best_P = P\n",
    "    lost_patience = 0\n",
    "    it = 0\n",
    "    validation_losses=[]\n",
    "    train_losses=[]\n",
    "    clf_p = Ridge(reg_lambda, fit_intercept=False, solver= 'auto')\n",
    "    clf_q = Ridge(reg_lambda, fit_intercept=False, solver= 'auto')\n",
    "    \n",
    "    idx_x = non_zero_idx[:,0]\n",
    "    idx_y = non_zero_idx[:,1]\n",
    "    value = M[idx_x, idx_y]\n",
    "    value = np.asarray(value).reshape(-1)\n",
    "    \n",
    "    idx_tuple = non_zero_idx.transpose()\n",
    "    idx_tuple = tuple(map(tuple, idx_tuple))\n",
    "    # print(value.shape)\n",
    "    \n",
    "    # start trainning\n",
    "    while lost_patience<patience:\n",
    "        \n",
    "        it += 1\n",
    "        current_train_loss = loss(value, idx_tuple, Q, P, reg_lambda)\n",
    "        train_losses.append(current_train_loss)\n",
    "        \n",
    "        if it%eval_every==0:\n",
    "            current_val_loss = loss(val_values, val_idx, Q, P, reg_lambda)\n",
    "            validation_losses.append(current_val_loss)\n",
    "            \n",
    "        if len(train_losses)>=2 and train_losses[-1]>=train_losses[-2]:\n",
    "            lost_patience += 1\n",
    "        else:\n",
    "            lost_patience = 0\n",
    "            \n",
    "        if len(train_losses)>=2 and train_losses[-1]<train_losses[-2]:\n",
    "            best_Q = Q\n",
    "            best_P = P       \n",
    "        \n",
    "        # update Q\n",
    "        clf_q.fit(P.transpose(), M.transpose())\n",
    "        Q = clf_q.coef_\n",
    "        \n",
    "        # update P\n",
    "        clf_p.fit(Q, M)\n",
    "        P = clf_p.coef_.transpose()\n",
    "        \n",
    "        converged_after = it - patience*eval_every\n",
    "        print(\"Iteration %s,   trainning loss %.2f,   validation loss %.2f\" % (it,\n",
    "                                                                       train_losses[-1],\n",
    "                                                                       validation_losses[-1]))\n",
    "    \n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the latent factor (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1,   trainning loss 2218582.81,   validation loss 3019.77\n",
      "Iteration 2,   trainning loss 1577870.02,   validation loss 3532.65\n",
      "Iteration 3,   trainning loss 1306264.02,   validation loss 3733.20\n",
      "Iteration 4,   trainning loss 1266097.77,   validation loss 3839.26\n",
      "Iteration 5,   trainning loss 1255187.13,   validation loss 3880.03\n",
      "Iteration 6,   trainning loss 1251088.68,   validation loss 3893.93\n",
      "Iteration 7,   trainning loss 1249164.49,   validation loss 3897.90\n",
      "Iteration 8,   trainning loss 1248111.33,   validation loss 3898.21\n",
      "Iteration 9,   trainning loss 1247467.32,   validation loss 3897.06\n",
      "Iteration 10,   trainning loss 1247035.46,   validation loss 3895.21\n",
      "Iteration 11,   trainning loss 1246722.09,   validation loss 3893.01\n",
      "Iteration 12,   trainning loss 1246480.16,   validation loss 3890.59\n",
      "Iteration 13,   trainning loss 1246284.90,   validation loss 3888.04\n",
      "Iteration 14,   trainning loss 1246122.53,   validation loss 3885.41\n",
      "Iteration 15,   trainning loss 1245984.76,   validation loss 3882.73\n",
      "Iteration 16,   trainning loss 1245866.22,   validation loss 3880.01\n",
      "Iteration 17,   trainning loss 1245763.15,   validation loss 3877.28\n",
      "Iteration 18,   trainning loss 1245672.77,   validation loss 3874.53\n",
      "Iteration 19,   trainning loss 1245592.94,   validation loss 3871.77\n",
      "Iteration 20,   trainning loss 1245521.97,   validation loss 3869.01\n",
      "Iteration 21,   trainning loss 1245458.48,   validation loss 3866.24\n",
      "Iteration 22,   trainning loss 1245401.41,   validation loss 3863.48\n",
      "Iteration 23,   trainning loss 1245349.86,   validation loss 3860.71\n",
      "Iteration 24,   trainning loss 1245303.14,   validation loss 3857.94\n",
      "Iteration 25,   trainning loss 1245260.66,   validation loss 3855.16\n",
      "Iteration 26,   trainning loss 1245221.96,   validation loss 3852.38\n",
      "Iteration 27,   trainning loss 1245186.64,   validation loss 3849.60\n",
      "Iteration 28,   trainning loss 1245154.39,   validation loss 3846.81\n",
      "Iteration 29,   trainning loss 1245124.92,   validation loss 3844.01\n",
      "Iteration 30,   trainning loss 1245098.00,   validation loss 3841.21\n",
      "Iteration 31,   trainning loss 1245073.40,   validation loss 3838.40\n",
      "Iteration 32,   trainning loss 1245050.95,   validation loss 3835.59\n",
      "Iteration 33,   trainning loss 1245030.47,   validation loss 3832.78\n",
      "Iteration 34,   trainning loss 1245011.80,   validation loss 3829.97\n",
      "Iteration 35,   trainning loss 1244994.81,   validation loss 3827.16\n",
      "Iteration 36,   trainning loss 1244979.36,   validation loss 3824.34\n",
      "Iteration 37,   trainning loss 1244965.33,   validation loss 3821.53\n",
      "Iteration 38,   trainning loss 1244952.61,   validation loss 3818.73\n",
      "Iteration 39,   trainning loss 1244941.10,   validation loss 3815.93\n",
      "Iteration 40,   trainning loss 1244930.70,   validation loss 3813.13\n",
      "Iteration 41,   trainning loss 1244921.33,   validation loss 3810.34\n",
      "Iteration 42,   trainning loss 1244912.90,   validation loss 3807.56\n",
      "Iteration 43,   trainning loss 1244905.34,   validation loss 3804.79\n",
      "Iteration 44,   trainning loss 1244898.57,   validation loss 3802.02\n",
      "Iteration 45,   trainning loss 1244892.55,   validation loss 3799.27\n",
      "Iteration 46,   trainning loss 1244887.20,   validation loss 3796.52\n",
      "Iteration 47,   trainning loss 1244882.47,   validation loss 3793.79\n",
      "Iteration 48,   trainning loss 1244878.31,   validation loss 3791.06\n",
      "Iteration 49,   trainning loss 1244874.68,   validation loss 3788.35\n",
      "Iteration 50,   trainning loss 1244871.54,   validation loss 3785.65\n",
      "Iteration 51,   trainning loss 1244868.83,   validation loss 3782.96\n",
      "Iteration 52,   trainning loss 1244866.54,   validation loss 3780.28\n",
      "Iteration 53,   trainning loss 1244864.61,   validation loss 3777.62\n",
      "Iteration 54,   trainning loss 1244863.03,   validation loss 3774.97\n",
      "Iteration 55,   trainning loss 1244861.77,   validation loss 3772.33\n",
      "Iteration 56,   trainning loss 1244860.79,   validation loss 3769.70\n",
      "Iteration 57,   trainning loss 1244860.08,   validation loss 3767.09\n",
      "Iteration 58,   trainning loss 1244859.60,   validation loss 3764.49\n",
      "Iteration 59,   trainning loss 1244859.35,   validation loss 3761.91\n",
      "Iteration 60,   trainning loss 1244859.30,   validation loss 3759.33\n",
      "Iteration 61,   trainning loss 1244859.43,   validation loss 3756.78\n",
      "Iteration 62,   trainning loss 1244859.73,   validation loss 3754.23\n",
      "Iteration 63,   trainning loss 1244860.18,   validation loss 3751.70\n",
      "Iteration 64,   trainning loss 1244860.76,   validation loss 3749.19\n",
      "Iteration 65,   trainning loss 1244861.48,   validation loss 3746.68\n",
      "Iteration 66,   trainning loss 1244862.30,   validation loss 3744.20\n",
      "Iteration 67,   trainning loss 1244863.23,   validation loss 3741.72\n",
      "Iteration 68,   trainning loss 1244864.25,   validation loss 3739.26\n",
      "Iteration 69,   trainning loss 1244865.35,   validation loss 3736.82\n",
      "Iteration 70,   trainning loss 1244866.53,   validation loss 3734.38\n"
     ]
    }
   ],
   "source": [
    "Q, P, val_loss, train_loss, converged = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                               k=100, val_idx=val_idx,\n",
    "                                                                               val_values=val_values_shifted, \n",
    "                                                                               reg_lambda=1e-4, init='random',\n",
    "                                                                               max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am sure there is sth wrong in the model, but I can't find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the validation and training losses over for each iteration (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAFhCAYAAAAbRiwDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3XmcneP9//HX+8xMFmRBYslCELsSxL4ktYbWUrXWEqqlLVXFt+XXFuWrulm+aBUVW2urFqG2qApqjSUhlorQiIQksghZZibz+f1xXyeO6Tkzk2Qmk3Pm/Xw8zsM5133d932d4bp9zrUqIjAzMzOz8pRr7wKYmZmZ2dJzMGdmZmZWxhzMmZmZmZUxB3NmZmZmZczBnJmZmVkZczBnZmZmVsYczJmVOUk3Svrf9i5HKZJ2k/RWe5ejOZLWkfSppKqlPP9TSeuvSGVagvsMlTS5Le9hZm3HwZxZmZD0uKRZkjo3kafd/6csKSQNzH+OiCcjYuP2LFMxkt6TtFf+c0RMiohVImLR0lwvnTtxRSrT8iSpk6S70ncISUMbHZekX0n6OL1+LUkFxwdJelHSvPTPQcv9S5iVKQdzZmVA0gBgNyCAA9vwPtVtdW3rEJ4CjgE+LHLsJOBgYCtgS+CrwMmQBYLAvcCfgFWBm4B7U7qZNcPBnFl5OA54FrgRGF4sg6SVgQeBPqlr7lNJfSTlJJ0t6Z3UInKnpNXSOQNSK8qJkiYBjxWkDZc0SdIMST8puM/2kp6RNFvSVElX5f+nK+mJlG1suv8RjVsLU8vNWZLGSZoj6Q5JXQqO/yhdd4qkbzVu6Wv0nftIGilppqQJkr5dcOz81FJ0h6S5kl6StFU6dguwDnBfKuePCr53dcrzuKT/lfR0ynOfpNUl/VnSJ5JeSEF2/n4haWAq06cFr3mSIuXZQNJj6d/DjHStnktQpua+752Sbk7fd7ykwaX/kypN0mmSXpfUr6XnRERtRFweEU8BxVoShwOXRMTkiPgAuAQ4Ph0bClQDl0fEwoi4AhCwx9KU36yjcTBnVh6OA/6cXvtKWrNxhoj4DNgPmJK65laJiCnAaWQtIkOAPsAs4HeNTh8CbArsW5C2K7AxsCdwrqRNU/oi4IdAL2CndPx7qQy7pzxbpfvfUeL7HA4MA9Yja6U5HkDSMOAMYC9gYCpXU24DJqfvdSjwC0l7Fhw/CPgLsBpwK3CPpJqIOBaYBByQyvnrEtc/EjgW6AtsADwD3JCu9wZwXuMTIqLw778KcDdwezos4OJU3k2B/sD56byWlKm573tguldPYCRwVYnvVZKkn5H9+xgSEZOVjdub3cTrGy289ObA2ILPY1Na/ti4+OL+kuMKjptZExzMma3gJO0KrAvcGREvAu8ALf0fKGRdWT9JLSILyYKHQxt1qZ4fEZ9FxPyCtJ9HxPyIGEv2P96tACLixYh4NiLqI+I94BqaD7oauyIFPTOB+4D8+KjDgRsiYnxEzAN+XuoCkvqTBZw/jogFEfEK8Eey4CvvxYi4KyLqgEuBLsCOS1DOGyLinYiYQ9bq+U5EPBoR9WRB4tZNnSzpx8AmwDcBImJCRIxKrU/TU5la9Ldr4fd9KiIeSGPsbiH9O2shSbqULKD/cipfftxezyZet7bw+qsAcwo+zwFWkaQix/LHuy1B+c06LI+PMVvxDQceiYgZ6fOtKe2yFp6/LnC3pIaCtEVAYeve+0XOKxz3NI/sf7hI2ogsCBkMrET2HHmxhWUpde0+6X0fYEwz5crrA8yMiLkFaf9J5fqv8yOiIXX39qHlPip4P7/I51VKnShpP+AHwA75IFnSGsAVZOMfu5H9oJ7VwrK05Ps2/rt2kVSdgs/m9CQb13ZECl5b26dA94LP3YFPIyIkNT6WPz4XM2uWW+bMVmCSupK1Vg2R9KGkD8m6OLfKj/9qJIqkvQ/s16g1pUsat9TUeaVcDbwJbBgR3YH/R9Z92BqmAoXjtPo3kXcKsJqkwtabdYDC77X4fEm5dO0pKWlJvvMSkbQx2SD+wyOiMCC9ON13y/S3O4Yv/u2aKlNLvu+ymEU2KeEGSbvkE/X58iilXke38Prj+WJL4VYpLX9sy9RKl7dlwXEza4KDObMV28FkrWibkXVFDiIba/Uk2Ti6xj4CVpfUoyDtD8BFktYFkNRb0kHLUKZuwCfAp5I2Ab5bpAxLu97ancAJkjaVtBJwbqmMKUh6GrhYUhdJWwInko0rzNtW0iGpS/l0YCHZRJJlLWdJkrqTzcz8aZoMUKgbWQvVbEl9gf9pdLxkmVr4fZsq142SbmwqT0Q8DhxN1pK7Q0rLL49S6rX4/pI66/PJLJ1SOfMB2s3AGZL6SuoDnEk2oQfgcbL/zk9L1zg1pT/Wku9m1tE5mDNbsQ0nG7c1KSI+zL/IBrYf3WjcGxHxJtkg+YlpcHof4P/IBsM/ImkuWTCzwzKU6SyyMXtzgeuAxpMczgduSvc/fEkuHBEPknVD/hOYQDbhALIgrJijgAFkrVZ3A+dFxKiC4/cCR5C1Oh0LHJLGz0HWSvbTVM6zlqSczdiGbOLIpYUtWOnYz9PxOcDfgb81Ore5MjX3fZvSH/hXc5nS9U4ARkratoXXznuLrPu5L/Bwer9uOnYN2fjIV4HXyL7/NemetWQ/XI4DZpONMTw4pZtZM/TFyUNmZiuONIP2NaBzC8d9FZ57PjAwIo5pi7KVE2VLx4wl696tay6/mZUXt8yZ2QpF0teU7SawKvAr4L4lDeTsi9IacJs6kDOrTA7mzGxFczIwnWwJlkX895g8MzMr4G5WMzMzszLmljkzMzOzMuZgzszMzKyMOZgzMzMzK2MO5szMzMzKmIM5MzMzszLmYM7MzMysjDmYsxWWpKq0FdI6rZl3Kcrxv83taWlmHYekAZIiv52epAclDW9J3qW41/+T9MdlKW+J6x4vqfHewVamHMxZqynch1JSg6T5BZ+PXtLrRcSitJH3pNbMa2Ydm6SHJV1QJP0gSR8uaeAVEftFxE2tUK6hkiY3uvYvIuJby3ptq2wO5qzVpGBqlYhYBZgEHFCQ9ufG+Zf2l6qZ2TK6EThWkhqlHwv82dvHWblxMGfLTequvEPSbZLmAsdI2knSs5JmS5oq6QpJNSl/deqeGJA+/ykdf1DSXEnPSFpvSfOm4/tJ+rekOZKulPQvSce38HscLGl8KvNjkjYuOPb/JE2R9ImkNyUNTek7SnoppX8k6Tet8Cc1s6VzD7AasFs+Ie0F/FXg5vT5K5JeTnX2fUnnl7qYpMclfSu9r5L0W0kzJE0EvtIo7wmS3kjPpYmSTk7pKwMPAn0KejT6SDpf0p8Kzj+w4PnzuKRNC469J+ksSePSs+0OSV1a8geRtLOkF9J5L0jaueDY8amscyW9m+9pkTRQ0uh0zgxJd7TkXtb6HMzZ8vY14FagB3AHUA/8AOgF7AIMI9ubs5RvAD8jexBPAi5c0ryS1gDuBP4n3fddYPuWFD49OP8EfB/oDTwK3CepRtLmqezbRER3YL90X4Argd+k9IHAXS25n5m1voiYT/YMOK4g+XDgzYgYmz5/lo73JAvIvivp4BZc/ttkQeHWwGDg0EbHp6Xj3YETgMskbRMRn5E9M6YU9GhMKTxR0kbAbcDpZM+fB8ieP50afY9hwHrAlsDxzRVY0mrA34ErgNWBS4G/S1o9BZlXAPtFRDdgZ+CVdOqFwCPAqkA/suectQMHc7a8PRUR90VEQ0TMj4gXIuK5iKiPiInAtcCQJs6/KyLGREQd8Gdg0FLk/SrwSkTcm45dBsxoYfmPBEZGxGPp3F+SPZR3IAtMuwCbS6qOiHfTdwKoAzaUtHpEzI2I51p4PzNrGzcBh0nqmj4fl9IAiIjHI+LV9KwaRxZENfVsyjscuDwi3o+ImcDFhQcj4u8R8U5kRpMFQ7sVu1ARRwB/j4hR6fnzW6ArWYCVd0VETEn3vo+mn5F5XwHejohb0rP4NuBN4IB0vAHYQlLXiJgaEeNTeh2wLtAnIhZEhCdUtBMHc7a8vV/4QdImkv6eBh1/AlxA1lpWyocF7+cBqyxF3j6F5YiIAL4w6LgJfYD/FJzbkM7tGxFvAWeSfYdpqTt5rZT1BGAz4C1Jz0vav4X3M7M2kAKP6cBBktYHtiPrNQBA0g6S/ilpuqQ5wHdo+tmU94XnCwXPi3Td/dLQkpmSZgP7t/C6+Ws3fv68D/QtyLMkz8ii1y0od9/UYngE2fefmp7Xm6Q8PwIEPJ+6fr/Zwu9hrczBnC1v0ejzNcBrwMDUBXku2cOhLU0l6xIAQJL44sOwKVPIfonmz82la30AEBF/iohdyLo4qki/yiPirYg4ElgDuAT4a0vHsphZm7mZrEXuWOCRiPio4NitwEigf0T0AP5Ay55NU4H+BZ8XL5ckqTPwV7IWtTUjoidZV2n+uo2fj401fv4o3euDFpSrxddN1uHz59rDEbE3sDZZi911Kf3DiPh2RPQhG2Lye0kDl7EsthQczFl76wbMAT5L49GaGi/XWu4HtpF0gLIZtT8gG3/SEncCBypbQqCGbNzdXOA5SZtK+nJ6YM9Pr0UAko6V1Cv9kp5D9tBuaN2vZWZL6GZgL7Jxbo2XFukGzIyIBZK2JxuD2xJ3AqdJ6pcmVZxdcKwT0JmsRbBe0n7APgXHPwJWl9SjiWt/RdKe6flzJrAQeLqFZSvlAWAjSd9QNpnsCLKehPslrZkmXayc7vUpnz/XDpOU/2E8i+y5tmgZy2JLwcGctbczgeFkAdE1ZJMi2lT69X0E2SDfj4ENgJfJHlTNnTuerLxXkz2QhwEHpvErnYFfk42/+5BsUPBP06n7A28om8X7W+CIiKhtxa9lZksoIt4jC4RWJmuFK/Q94IJUZ88lC6Ra4jrgYWAs8BLwt4L7zQVOS9eaRRYgjiw4/ibZ2LyJabZqn0blfQs4hmyiwQyyMW0HLOuzJCI+JhtLfCbZM/FHwFcjYgZZnHAmWevdTLJxg99Lp25H9kP20/Q9fhAR7y5LWWzpKBsuZNZxSaoie1AdGhFPtnd5zMzMloRb5qxDkjRMUo/UJfozspmoz7dzsczMzJaYgznrqHYFJpJ1VQwDDo6IZrtZzczMVjTuZjUzMzMrY26ZMzMzMytjDubMzMzMylh1exdgeenVq1cMGDCgvYthZsvRiy++OCMiWrqG4ArNzzCzjmVJnl8dJpgbMGAAY8aMae9imNlyJKnxFkVly88ws45lSZ5f7mY1MzMzK2MO5szMzMzKmIM5MzMzszLmYM7MzMysjDmYMzMzMytjDubMzMzMypiDOTMzM7My5mDOzMzMrIw5mDMzMzMrYw7mirjn5Q94buLH7V0MM1uBSOoi6XlJYyWNl/TzlH6jpHclvZJeg1K6JF0haYKkcZK2KbjWcElvp9fw9vpOpcyeV8vD4z9kQd2i9i6KmbVAh9nOa0lc/OAbfHnjNdhh/dXbuyhmtuJYCOwREZ9KqgGekvRgOvY/EXFXo/z7ARum1w7A1cAOklYDzgMGAwG8KGlkRMxaLt+iGbPn1XLktc/y5odz6bVKJ47eYV2O3nEd1ujWpb2LZmYluGWuiOpcjrpF0d7FMLMVSGQ+TR9r0qupB8VBwM3pvGeBnpLWBvYFRkXEzBTAjQKGtWXZW2rugjqGj3ieiTM+47wDNmNQ/55c8djb7PrLf3LmnWOZNndBexfRzIpwMFdEp+ocdYsa2rsYZraCkVQl6RVgGllA9lw6dFHqSr1MUueU1hd4v+D0ySmtVHqx+50kaYykMdOnT2/V79LYvNp6vnnjC4yf8glXH70NJ+yyHn8cvh2PnTmUb+ywDn9/dQpf+93T/PujuW1aDjNbcg7miqjOifoGB3Nm9kURsSgiBgH9gO0lbQGcA2wCbAesBvw4ZVexSzSRXux+10bE4IgY3Lt372UufykL6hZx0s0v8uJ/ZvF/R27NnpuuufjYer1W5vwDN+cvJ+9M7aIGvv77p3nq7RltVhYzW3IO5oqoqXI3q5mVFhGzgceBYRExNXWlLgRuALZP2SYD/QtO6wdMaSK93Vx4/+s8NWEGvzl0K76y5dpF83ypXw/uOWUX+q7aleNveJ7bn5+0nEtpZqU4mCuipkruZjWzL5DUW1LP9L4rsBfwZhoHhyQBBwOvpVNGAselWa07AnMiYirwMLCPpFUlrQrsk9LazWNvTuMrW67N17ft12S+vj278pfv7MTOA3tx9t9e5dcPvUmEf/iatTfPZi2ipipHvVvmzOyL1gZuklRF9kP4zoi4X9JjknqTdZ++Anwn5X8A2B+YAMwDTgCIiJmSLgReSPkuiIiZy/F7fMG0uQuYOmcBW/fv2aL83brUMGL4YH5272v8/vF3+OiThfzy61+ipsptA2btxcFcEdVVotYtc2ZWICLGAVsXSd+jRP4ATilxbAQwolULuJTGvT8HgK1aGMwBVFfl+MXXvsRa3bty2aP/ZvqnC7n66G1YubP/l2LWHvxTqoisZc7BnJlVvnGTZ5MTbN6n+xKdJ4kf7LUhvzzkS/xrwgyOvPZZps9d2EalNLOmOJgrwhMgzKyjGDt5Dhut2Y2VOi1dq9qR26/Dtcduy9vT5nLI1f/y0iVm7cDBXBHVOU+AMLPKFxGMnTybLfv1WKbr7Lnpmtx+0k4sqGvga7/7F4++/lErldDMWsLBXBE11TnqG9wyZ2aV7f2Z85k9r44t+7V8vFwpg/r3ZOSpu7B+71X49i1j+P3jEzzT1Ww5cTBXRI1b5sysAxg7eTaQBWKtYe0eXbnz5J346pZ9+PVDb3H6Ha8wr7a+Va5tZqU5mCui2kuTmFkHMG7ybDpV59h4rW6tds2unaq44shBnLXPRowcO4WDrvI4OrO25mCuiJqqnJcmMbOKN3byHDZbu3urrxEniVP32JCbv7k9s+bVcuBVT3HHC5Pc7WrWRhzMFVFTJS9NYmYVbVFD8NoHc9hqGSc/NGW3DXvzwA92Y9t1V+XHf32V0+94hbkL6trsfmYdlYO5Irw0iZlVunemf8q82kWtMvmhKWt068LN39yBs/bZiPvGTmHY5U/y1Nsz2vSeZh1Ns8GcpBGSpkl6rSDtfEkfSHolvfZP6QMkzS9I/0PBOdtKelXSBElXpH0MkbSapFGS3k7/XDWlK+WbIGmcpG0KrjU85X9b0vDW/INAtgOEJ0CYWSUb+342+WGr/m3XMpdXlcu6Xe/67s50rslxzPXP8dN7XuWzhZ4cYdYaWtIydyMwrEj6ZRExKL0eKEh/pyD9OwXpVwMnARumV/6aZwP/iIgNgX+kzwD7FeQ9KZ2PpNWA84AdgO2B8/IBYGvpVOWlScysso2dPJtVOlezfq9Vlts9t1lnVR44bTe+tet6/Pm5Sex7+RM8PcGtdGbLqtlgLiKeAJZpE2hJawPdI+KZtF/hzcDB6fBBwE3p/U2N0m+OzLNAz3SdfYFRETEzImYBoygebC616lyORQ1BgwM6M6tQ4ybPYYu+3cnltFzv26Wmip9+dTPuPHknqnPiG398jjPueIWPP/VWYGZLa1nGzJ2auj9HNGoZW0/Sy5JGS9otpfUFJhfkmZzSANaMiKkA6Z9rFJzzfpFzSqW3muqq7OFW1+CuVjOrPAvrF/HG1E/Yqo3HyzVluwGr8dDpu3Pqlwdy37gp7HHJaG5/fpJ/RJsthaUN5q4GNgAGAVOBS1L6VGCdiNgaOAO4VVJ3oNhPv+ZqbKlzWnwtSSdJGiNpzPTp05u53ec6pWn6ngRhZpXozalzqVsUbNVKiwUvrS41VZy178Y8+IPd2Hitbpz9t1c57JpneO2DOe1aLrNys1TBXER8FBGLIqIBuI5s7BoRsTAiPk7vXwTeATYiaz3rV3CJfsCU9P6j1H2a746dltInA/2LnFMqvVg5r42IwRExuHfv3i3+fvmWOS9PYmaVaFza+WFZ92RtLQPX6MYdJ+3Irw/dkvdmfMYBVz3FT+5+lVmf1bZ30czKwlIFc/ngK/ka8FpK7y2pKr1fn2zywsTUfTpX0o5pFutxwL3p/JFAfkbq8Ebpx6VZrTsCc9J1Hgb2kbRq6t7dJ6W1mvwCml442Mwq0djJc1h95U707dm1vYuymCQOH9yfx84ayvE7D+D2F95n6G8f55Zn3vMPa7NmVDeXQdJtwFCgl6TJZDNJh0oaRNa9+R5wcsq+O3CBpHpgEfCdiMhPnvgu2czYrsCD6QXwS+BOSScCk4DDUvoDwP7ABGAecAJARMyUdCHwQsp3QcE9WkXN4pY5d7OaWeUZN3k2W/brQVohaoXSo2sN5x2wOUdtvw7njxzPz+4dz5+encTPvroZu27Yq72LZ7ZCajaYi4ijiiRfXyLvX4G/ljg2BtiiSPrHwJ5F0gM4pcS1RgAjSpd62eRb5hzMmVmlWdQQTJj2KXtvtmZ7F6VJG63ZjT9/awceHv8hFz3wBsdc/xx7bbomP/nKpqzXa+X2Lp7ZCsU7QBRR7W5WM6tQn8yvoyFg9ZU7t3dRmiWJYVuszagfDuFHwzbmmXdmsM9lo/nFA2/wibcFM1vMwVwRNWndpXovTWJmFWb2/CwI6rlSTTuXpOW61FTxvaED+edZQzl4UF+ue3Iie/z2cW5/fhKLvJSJmYO5YvLdrHX1fkiYWWWZPS+bIVpOwVzeGt278JvDtuLeU3ZhwOorc/bfXuXAq57i+Xdbddi0WdlxMFeEFw02s0o1J7XM9ehafsFc3pb9evKX7+zEFUdtzazPajn8mmc47baX+XDOgvYumlm7cDBXRCdPgDCzCvV5MNepnUuybCRx4FZ9+MeZQzltj4E8NP5D9rjkcX7/+AQW1i9q7+KZLVcO5oqoXrwDhFvmzKyyVELLXKGunao4Y5+NefSHQ9hlYC9+/dBb7HvZE4z+d8t3/TErdw7misivM+dgzswqzex5lRXM5a2z+kpcd9xgbvrm9khi+Ijn+d6fX2TqnPntXTSzNudgroga781qZhVq9rw6Vu5URafqynz8D9moNw+dvhtn7r0R/3hjGnteMpprRr/jH+dW0SqzNi8j781qZpVqzvy6imuVa6xzdRXf33NDHj1jCDtvsDoXP/gmB1z5FC9NmtXeRTNrEw7mivDerGZWqebMr6XHSuU9+aGl+q+2En8cvh3XHLstc+bX8fWrn+an97zqBYet4jiYK6Im59msZlaZZs+ro2eFt8w1tu/mazHqjCEcv/MAbn1uEnteMpoHXp1KtmukWflzMFdETbV3gDCzytQRulmLWaVzNecdsDn3nLILa3TrzPf+/BIn3/IiH33items/DmYK6I6l+9m9a82M6sss+fXleXuD61ly349ufeUXTh7v00Y/e/p7HXpaG57fpJb6aysOZgr4vNFg90yZ2aVIyKylrkOHMxBtpbod4ZswMOn787mfbpzzt9e5ajrnuU/H3/W3kUzWyoO5oqo9jpzZlaBFtQ1UFvf0CG7WYsZ0Gtlbvv2jlx8yJcY/8EnDLv8SUY89S6LGtxKZ+XFwVwRnwdzrtBmVjlmz68FoGeZb+XVmiRx1Pbr8MgZu7Pj+qtxwf2vc/g1z/DO9E/bu2hmLeZgroj8bFa3zJlZJam0rbxa09o9ujLi+O249PCtmDDtU/b7vye5ZvQ7bqWzsuBgrohcTlTl5KVJzKyi5Lfy6sgTIJoiiUO26ceoM3Zn6Ea9ufjBNzn0D08zYZpb6WzF5mCuhJoqUeelScysglTqvqytbY1uXbjm2G35vyMH8e6Mz9j/iie59gm30tmKy8FcCTW5HHX1rrhmVjk+cTdri0nioEF9eeSHuzNko9784oE3OewPTzPRY+lsBeRgroTqKnnRYDOrKIsnQLibtcXW6NaFa4/dlsuPGMQ707NWuhFPvUuDW+lsBeJgroSaqpwnQJhZRZkzv46qnFilc3V7F6WsSOLgrbNWul026MUF97/OkV6XzlYgDuZKyII5//Iys8oxe162lZek9i5KWVqzexf+OHwwvz1sK96Ymq1Ld8sz77mVztqdg7kSaqrkljkzqyiz59fR0+PllokkDt22H4/8cHe2W281fnbveI4d8RyTZ81r76JZB+ZgroTqqpyXJjGzivLJ/Dq6O5hrFWv36MpNJ2zHxYd8iVcmzWbY5U9yu/d4tXbiYK4Ej5kzs0oze16dJz+0ovzuEQ//cHe27NeDs//2KsNveIGpc+a3d9Gsg3EwV4K7Wc2s0syeX+tu1jbQb9WV+NOJO3DhQZvzwrsz2eeyJ/jLmPfdSmfLjYO5Eqpzot6DWs2sgsxJEyCs9eVy4tidBvDQ6bux6Vrd+Z+7xvGtm8bw0ScL2rto1gE4mCuhpipHbb1b5swsI6mLpOcljZU0XtLPGx2/UtKnBZ+PlzRd0ivp9a2CY8MlvZ1ew5dH+Rc1BJ8sqKfHSp2Wx+06rHVXX5nbT9qRc7+6GU9NmMHel47mby9NdiudtalmgzlJIyRNk/RaQdr5kj4oeEjtX3DsHEkTJL0lad+C9GEpbYKkswvS15P0XHqo3SGpU0rvnD5PSMcHNHeP1lRTlXPLnJkVWgjsERFbAYOAYZJ2BJA0GOhZ5Jw7ImJQev0x5V0NOA/YAdgeOE/Sqm1d+LkL0r6sbplrc7mc+Oau6/HgD3ZjwzW7ccadY/n2zS8yba5b6axttKRl7kZgWJH0ywoeUg8ASNoMOBLYPJ3ze0lVkqqA3wH7AZsBR6W8AL9K19oQmAWcmNJPBGZFxEDgspSv5D2W7Gs3z2PmzKxQZPItbzXpFen58xvgRy281L7AqIiYGRGzgFEUf8a2Ku/Luvyt33sV7jx5J376lU158u3p7H3pE9zz8gdupbNW12wwFxFPADNbeL2DgNsjYmFEvAtMIPvluT0wISImRkQtcDtwkLKVK/cA7krn3wQcXHCtm9L7u4A9U/5S92hV1V402MwaST9OXwGmkQVkzwGnAiMjYmqRU74uaZykuyT1T2l9gfcL8kxOaW1qdtqX1bNZl6+qnPjWbuvzwA92Y4PeK3P6Ha9w0i0vMs1j6awVLcuYuVPTQ2pEQRdBqYdUqfTVgdkRUd8o/QvXSsfnpPwtfhBKOknSGEljpk+fvkRfrlNVjnq3zJlZgYhYFBG32828AAAgAElEQVSDgH7A9pJ2Bw4DriyS/T5gQERsCTzK5z9Oi22/UPSX47I8wxqb42CuXW3QexX+8p2d+cn+m/LEv6ez92VPcPfLHktnrWNpg7mrgQ3Ixo1MBS5J6aUeUkuavjTX+u/EiGsjYnBEDO7du3exLCVVu5vVzEqIiNnA48CXgYHABEnvAStJmpDyfBwRC9Mp1wHbpveTgf4Fl+sHTClxn6V+hjU2e14t4G7W9lSVE9/e/fNWuh/eMdYzXq1VLFUwFxEfpV+oDWQPqXw3Z6mHVKn0GUBPSdWN0r9wrXS8B1l3b4sfhMuiOuduVjP7nKTeknqm912BvYAXI2KtiBgQEQOAeWmcL5LWLjj9QOCN9P5hYB9Jq6ZejX1SWpvKt8z16OrZrO2tsJXuqQkz2OvS0dzpdelsGSxVMNfoIfU1ID/TdSRwZJqJuh6wIfA88AKwYZq52olsAsPIyP7L/SdwaDp/OHBvwbXyU/YPBR5L+Uvdo1V1qnbLnJl9wdrAPyWNI3umjYqI+5vIf1pawmQscBpwPEBEzAQuTNd4AbggpbWpOZ4AsULJt9I9dPrubLpWd3501ziOv+EFpsz27hG25KqbyyDpNmAo0EvSZLIp9UMlDSLr3nwPOBkgIsZLuhN4HagHTomIRek6p5L9+qwCRkTE+HSLHwO3S/pf4GXg+pR+PXBL6rKYSRYANnmP1lSd89IkZva5iBgHbN1MnlUK3p8DnFMi3whgRKsWsBmz59exUqcqOlV7edEVyXq9snXpbn7mPX710Fvsc9kTnLP/Jhy13TrkcsVGFZn9t2aDuYg4qkjy9UXS8vkvAi4qkv4A8ECR9IkUmY0aEQvIBha3+B6tqaYqR50XDTazCjFnfp3XmFtB5XLi+F3WY49N1uTsv43jJ3e/xv1jp/LLr3+JdVdfub2LZ2XAP9FKqKkSdQ0O5sysMsyeV0d3B3MrtHVWX4k/f2sHLj7kS7z2wRz2vfwJ/vjkRBa5l8ia4WCuhJqqHPWeAGFmFWLO/FovS1IGJHHU9uvwyBm7s/MGvfjfv7/BoX94mrc/mtveRbMVmIO5EqqrRH1DeHaRmVWErJvVM1nLxdo9unL98MFcfsQg3pvxGV+54imu+Mfb3jPcinIwV0JNVfan8fIkZlYJZs+r80zWMiOJg7fuy6gzhrDvFmtx6ah/c+BVTzFu8uz2LpqtYBzMlVBTlc0i8vIkZlYJZs+vczdrmeq1SmeuPGprrjtuMLPm1XLw7/7FRX9/nfm1rb6Qg5UpB3MlVOeyP43HzZlZuVtQt4ja+gZPgChze2+2JqPOGMIR263DdU++y76XP8G/Jsxo72LZCsDBXAk1aS0mz2g1s3I3e573Za0U3bvUcPEhX+L2k3YkJzj6j8/xo7vGLl4U2jomB3Ml1OTczWpmlSG/lZcnQFSOHddfnYdO353vDNmAv770AXteOpq/j5vqSXsdlIO5EvITINzNamblbva8WsBbeVWaLjVVnL3fJtx7yi6s1aMzp9z6Et+++UWmzvGWYB2Ng7kSqtMEiFq3zJlZmZs9392slWyLvj2453u78JP9N+WpCdPZ+9InuPmZ92jwYsMdhoO5EtwyZ2aVIt/N6pa5ylVdlePbu6/PI6cPYVD/npx773gO/cPT/NuLDXcIDuZK+HydObfMmVl5yw+O7+GWuYq3zuorccuJ23Pp4Vvx7ozP+MoVT3LpI2+xoM7LmFQyB3MlVHudOTOrELPn11KVE906V7d3UWw5kMQh2/Tj0TOG8NUt+3DFYxPY/4oneXbix+1dNGsjDuZK6JTvZvWYAzMrc3PmZ7s/SGrvothytPoqnbnsiEHc/M3tqVvUwJHXPsvZfx3nZUwqkIO5EqrzS5N4HzwzK3Peyqtj232j3jxy+hBOHrI+f3lxMnteOpqRY6d4GZMK4mCuhOr8mDm3zJlZmcu3zFnH1bVTFefstykjT92FPj27cNptL3PCjS/w/sx57V00awUO5krId7O6Zc7Myp2DOcvbvE8P7v7eLpx3wGa88O5M9rnsCa594h3qPT68rDmYKyE/AaLe23mZWZmbPa/Oa8zZYlU5ccIu6zHqjCHsMrAXv3jgTQ686l+MfX92exfNlpKDuRLyS5PUep05Mytzc+bX0dMtc9ZIn55due64bbn66G34+LOFfO33/+L8keP5dGF9exfNlpCDuRJq8i1zbno2szK2qCH4ZIG7Wa04Sez3pbUZdcYQjtlxXW565j32umQ0D4//sL2LZkvAwVwJ3gHCzCrB3AV1RECPlTq1d1FsBda9Sw0XHLQFf/3uzvRcqYaTb3mRk24ew5TZ3ue1HDiYK8F7s5pZJchv5eVuVmuJbdZZlfu+vyvn7LcJT7w9nb0vHc2Ip95lkVd2WKE5mCuhJpdvmXMwZ2bla/Y878tqS6amKsfJQzZg1A+HsN16q3HB/a9z8O/+xauT57R30awEB3Ml1FTn92b1rxEzK19zF2SD2bs7mLMl1H+1lbjh+O343Te24cNPFnDQ757i5/d5gsSKyMFcCYt3gPDSJGZWxhbWZxusd672496WnCS+suXaPHrGEI7eYV1ufDqbIPHQax96B4kViGt3CTWLFw32f6xmVr5q08Ln+Wea2dLo0bWGCw/egr+lCRLf+dOLfPvmMXzgCRIrBNfuEqpyIicvGmxm5S0/iauTW+asFWydJkj8v/034V8TPmbvS0dz3RMTPb68nbl2N6GmKucxc2ZW1vItc+5mtdZSU5XjpN03YNQZu7PzBqtz0QNvcMBV/+LlSbPau2gdVrO1W9IISdMkvVbk2FmSQlKv9HmopDmSXkmvcwvyDpP0lqQJks4uSF9P0nOS3pZ0h6ROKb1z+jwhHR9QcM45Kf0tSfsu25+gtCyY868NMytfbpmzttJv1ZW47rjB/OGYbZn1WS2HXP00P73n1cXL4djy05LafSMwrHGipP7A3sCkRoeejIhB6XVBylsF/A7YD9gMOErSZin/r4DLImJDYBZwYko/EZgVEQOBy1I+0nlHApuncv0+Xb/VVVfJTcdmVtbyLXOdPGbO2oAkhm2xFo+eOYQTdl6PW5+bxF6Xjmbk2CmeILEcNVu7I+IJYGaRQ5cBPwJa8m9re2BCREyMiFrgduAgSQL2AO5K+W4CDk7vD0qfScf3TPkPAm6PiIUR8S4wIV2/1dVU5bw3q5mVtcUTINwyZ21olc7VnHvAZow8dVfW7tGF0257meNGPM97Mz5r76J1CEtVuyUdCHwQEWOLHN5J0lhJD0raPKX1Bd4vyDM5pa0OzI6I+kbpXzgnHZ+T8pe6VqurybllzszKm1vmbHnaom8P7v7eLlxw0Oa8Mmk2+1z+BFf+4+3FS+RY21ji2i1pJeAnwLlFDr8ErBsRWwFXAvfkTyuSN5pIX9pzGpf1JEljJI2ZPn16sSxNqqn2mDkzK2/5Z1hNVbFHp1nrq8qJ43YawKNnDmGfzdbkklH/Zr//e5Kn35nR3kWrWEvzU20DYD1grKT3gH7AS5LWiohPIuJTgIh4AKhJkyMmA/0LrtEPmALMAHpKqm6UTuE56XgPsu7eUtf6LxFxbUQMjojBvXv3XuIvWp0Tdd6PzszK2MJFDXSqzpGNUjFbftbs3oWrvrENN56wHfWLgm9c9xxn3PEKMz5d2N5FqzhLHMxFxKsRsUZEDIiIAWTB1TYR8aGktdK4NiRtn67/MfACsGGaudqJbALDyMhGR/4TODRdfjhwb3o/Mn0mHX8s5R8JHJlmu64HbAg8v8TfvAVqqnLuZjWzslZb30Bnd7FaOxq68Ro88sPdOfXLA7lv3BT2vGQ0tz43iQY3lrSalixNchvwDLCxpMmSTmwi+6HAa5LGAlcAR0amHjgVeBh4A7gzIsanc34MnCFpAtmYuOtT+vXA6in9DOBsgHTencDrwEPAKRHRJp3xXmfOzMpdbX2DlyWxdtelpoqz9t2YB3+wG5uu3Y3/d/erHPqHp3l9yiftXbSKUN1chog4qpnjAwreXwVcVSLfA8ADRdInUmQ2akQsAA4rca2LgIuaKldrqK6Sx8yZWVmrrW/wVl62whi4Rjdu+/aO/O2lD9Jiw0/xzV0GcPpeG7Fy52ZDEivBNbwJXjTYzMpd7SK3zNmKRRJf37Yfj505hMMH9+e6J99lr0tH89BrH3ptuqXkGt6EmipR725WMytjdQ7mbAXVc6VOXHzIl/jrd3eiR9cavvOnFznxpjG8P3Neexet7LiGN8Etc2ZW7mrrG7zGnK3Qtl13Ne7//q789Cub8uzEj9n7stH87p8TFq+RaM1zDW9Cdc4TIMysvC30BAgrA9VVOb612/o8esYQhm60Br95+C32v+JJnp34cXsXrSy4hjehpkrUN/iXgZmVL89mtXLSp2dX/nDstow4fjAL6hZx5LXPcsadXpuuOa7hTfDSJGZW7moXuZvVys8em6zJqB8O4ZQvb8B9Y7O16f783H+8Nl0JruFN8NIkZlbu3DJn5aprpyr+Z99NFq9N95O7X+NrVz/Nax/Mae+irXBcw5vQyRMgzKzM1bllzspcfm26y47Yig9mzePAq57i/JHjmbugrr2LtsJwDW9CtZcmMbMy55Y5qwSS+NrW/fjHGUM5eod1uemZ99jzktHcN3aK16bDwVyTaqpy1LplzswASV0kPS9prKTxkn7e6PiVkj4t+NxZ0h2SJkh6TtKAgmPnpPS3JO3bluV2MGeVpMdKNVx48Bbc871dWLN7F75/28sce/3zTJz+afMnVzDX8CbUVOXcMmdmeQuBPSJiK2AQMEzSjgCSBgM9G+U/EZgVEQOBy4BfpbybAUcCmwPDgN9LqmqrQnsHCKtEW/XvyT2n7MIFB23O2MmzGXb5k1z6yFssqGuTrdpXeK7hTajOeWkSM8tEJv/zvya9IgVivwF+1OiUg4Cb0vu7gD0lKaXfHhELI+JdYAJF9qduLQu9aLBVqKqcOG6nAfzjzCF8Zcu1ueKxCex92Wj++ea09i7acuca3oT80iTujzczAElVkl4BpgGjIuI54FRgZERMbZS9L/A+QETUA3OA1QvTk8kprU24m9Uq3RrdunDZEYO49ds70Kkqxwk3vsDJt4zhg9nz27toy41reBNqqgRAvde1MTMgIhZFxCCgH7C9pN2Bw4Ari2RXsUs0kf7fF5BOkjRG0pjp06cvVZk9m9U6ip036MWDP9idHw3bmNH/ns5el4zmD6Pf6RCrUriGN6EmPQA7wn8IZh2NpA0kdU7vh0o6TVLjcW9FRcRs4HHgy8BAYIKk94CVJE1I2SYD/dP1q4EewMzC9KQfMKXEfa6NiMERMbh3795L+A2hflEDDYFb5qzD6FSd43tDBzLqh0PYdcNe/PLBN9n//yp/WzDX8CZULw7m3DJnVoH+CiySNBC4HlgPuLVUZkm988GepK7AXsCLEbFWRAyIiAHAvDThAWAkMDy9PxR4LLIxGyOBI9Ns1/WADYHnW//rsXg2voM562j6r7YS1x03mD8eN5h5tWlbsDteYfrcytwWrLq9C7Ai65TvZnXLnFklaoiIeklfAy6PiCslvdxE/rWBm9KEhxxwZ0Tc30T+64FbUkvdTLIZrETEeEl3Aq8D9cApEdEmU/Bq61Mw525W66D22mxNdhnYi9/9cwLXPPEOo974iLP22ZhjdlyXqlyxEQ/lycFcE9wyZ1bR6iQdRdZ6dkBKqymVOSLGAVs3dcGIWKXg/QKy8XTF8l0EXLSkBV5S+WCuxi1z1oF17VTFWftuzNe26cu5977GeSPH85cX3+fCg7Zg63VWbe/itQrX8CZUp6jdY+bMKtIJwE7ARRHxbury/FM7l6lV5btZO7tlzowNeq/Cn07cgSuO2pppnyzkkKuf5py/jWPWZ7XtXbRl5pa5JuTHmTiYM6s8EfE6cBqApFWBbhHxy/YtVeta3M3qljkzINsW7MCt+vDljXtz+aNvc+PT7/HQax9y9n6bcNi2/cmVadera3gTqnPZn8dLk5hVHkmPS+ouaTVgLHCDpEvbu1ytyRMgzIrr1qWGn311M+7//q4MXGMVfvzXVzn0D08zfsqc9i7aUnENb0J+nbn8r1szqyg9IuIT4BDghojYlmyGasXwBAizpm26dnfuOGknfnvYVvzn43kccOVTnD9yPJ8sqGvvoi0R1/Am5NeZc8ucWUWqlrQ2cDjQ1KzUsuUJEGbNy+XEodv247Ezh3L0Duty0zPvscdvR3P3y5PLZgco1/AmLA7mPGbOrBJdADwMvBMRL0haH3i7ncvUqtwyZ9ZyPVaq4cKDt2DkKbvSd9Wu/PCOsRxx7bO89eHc9i5as1zDm1Cd72Z1MGdWcSLiLxGxZUR8N32eGBFfb+9ytSaPmTNbcl/q14O7v7szFx/yJf790Vz2v+JJLvr763y6sL69i1aSa3gTFu/N6nXmzCqOpH6S7pY0TdJHkv4qqV97l6s15VvmOjuYM1siuZw4avt1eOzMoRw+uB/XPfkue17yOPeNnbJCdr26hjfBe7OaVbQbyLbW6gP0Be5LaRXDLXNmy2a1lTtx8SFbcvf3dqZ3t858/7aXOfqPzzFh2orV9eoa3oT80iTeAcKsIvWOiBsioj69bgSWfDf7FZjHzJm1jq3XWZV7T9mVCw/egtc+mMOwy5/k4gff4LMVpOu1RTVc0ojUFfFakWNnSQpJvdJnSbpC0gRJ4yRtU5B3uKS302t4Qfq2kl5N51whSSl9NUmjUv5RaWHPJu/RmjpVewcIswo2Q9IxkqrS6xjg4/YuVGvybFaz1lOVE8fuuC7/PGsoX9u6L9eMnshel47m7+OmtnvXa0tr+I3AsMaJkvoDewOTCpL3AzZMr5OAq1Pe1YDzgB2A7YHz8sFZynNSwXn5e50N/CMiNgT+kT6XvEdr+3zRYAdzZhXom2TLknwITAUOJdviq2Is7mZ1y5xZq1l9lc785rCt+Ot3d2bVlTpxyq0vccz1zzFh2qftVqYW1fCIeAKYWeTQZcCPgMKQ9CDg5sg8C/RMazntC4yKiJkRMQsYBQxLx7pHxDORhbY3AwcXXOum9P6mRunF7tGqaqrdzWpWqSJiUkQcGBG9I2KNiDiYbAHhiuHtvMzazrbrrsp939+VCw/anFcnz2G//3ui3bpel7qGSzoQ+CAixjY61Bd4v+Dz5JTWVPrkIukAa0bEVID0zzWauUerqsm5m9WsgzmjvQvQmvItc57NatY2qnLi2J0G8FhB1+uel4zm/nHLd9brUtVwSSsBPwHOLXa4SFosRXqTRWjJOZJOkjRG0pjp06c3c8n/Vr140WC3zJl1EOW5y3YJngBhtnz0WqUzvz4063pdbeVOnHrry6nrdfnMel3aGr4BsB4wVtJ7QD/gJUlrkbWS9S/I2w+Y0kx6vyLpAB/lu0/TP6el9FLX+oKIuDYiBkfE4N69l3ySWn6dObfMmXUYFfXLrba+geqcyOUqKkY1W2E17noddvmT/OKBN9p8weGlCuYi4tU0xmRARAwgC662iYgPydZtOi7NON0RmJO6SB8G9pG0apr4sA/wcDo2V9KOaRbrccC96VYjgfys1+GN0ovdo1V9vs5cRT3fzTo0SXMlfVLkNZdszbmKUVvfsPg5ZmbLR2HX6yHb9OXaJyay5yWPM7INFxxu6dIktwHPABtLmizpxCayPwBMBCYA1wHfA4iImcCFwAvpdUFKA/gu8Md0zjvAgyn9l8Dekt4mmzX7y6bu0dq8aLBZ5YmIbhHRvcirW0RUt3f5WlPtogZPfjBrJ/mu17+lBYdPu+1lfn7f621yrxY9uCLiqGaODyh4H8ApJfKNAEYUSR8DbFEk/WNgzyLpJe/RmqpyQoJ6B3NmVobqHMyZtbtt0oLDtz0/iS379WiTe1TUr9C2UJPLUdfgblYzKz8L6xs8+cFsBVCVE8fsuG6bXd+1vBk1VaKu3i1zZlZ+ausbvCyJWQfgWt6M6qoc9W6ZM7MyVFvvblazjsC1vBk1VbnFC2+amZWT2kWezWrWEbiWN6OmSp4AYWZlyRMgzDoG1/Jm1FTlvM6cmZWlWk+AMOsQXMubUV0lrzNnZmXJY+bMOgbX8mbU5HLem9XMytJCB3NmHYJreTNqqt0yZ2blyTtAmHUMruXNqPaiwWZWpjxmzqxjcC1vRqeqnBcNNrOyVLfIwZxZR+Ba3ozqKlHf4GDOzMqPJ0CYdQyu5c3IFg12N6uZlR8Hc2Ydg2t5M7xosJmVK0+AMOsYXMubUe2lScysDDU0BHWLwmPmzDoA1/Jm1FTnvDSJmZWd/J7Sbpkzq3yu5c2oyYk6T4AwszKT/xHqljmzyuda3oyaqhx19e5mNbPyUlvvljmzjsK1vBlemsTMypG7Wc06DtfyZtRU5ajzBAgzKzOLW+bczWpW8VzLm1FT5b1Zzaz85IO5GrfMmVU81/JmVFd5aRIzKz8L3TJn1mG4ljcj2wGigQgHdGZWPvI9Cp3dMmdW8VzLm1GTEwCLGhzMmVn58GxWs47DtbwZ+fEmngRhZuXEs1nNOg7X8mZUp5Y5LxxsZuXEs1nNOg7X8mbkf9V6EoSZlZPFs1kdzJlVPNfyZlTn8t2sbpkzs/LhblazjqPZWi5phKRpkl4rSLtQ0jhJr0h6RFKflD5U0pyU/oqkcwvOGSbpLUkTJJ1dkL6epOckvS3pDkmdUnrn9HlCOj6g4JxzUvpbkvZtnT9FcdVVqZvVwZxZhyapi6TnJY2VNF7Sz1P69SltnKS7JK2S0o+XNL3gefitgmsNT8+8tyUNb4vy5lvmPJvVrPK1pJbfCAxrlPabiNgyIgYB9wPnFhx7MiIGpdcFAJKqgN8B+wGbAUdJ2izl/xVwWURsCMwCTkzpJwKzImIgcFnKRzrvSGDzVK7fp+u3ifx4E0+AMOvwFgJ7RMRWwCBgmKQdgR9GxFYRsSUwCTi14Jw7Cp6HfwSQtBpwHrADsD1wnqRVW7uwbpkz6ziareUR8QQws1HaJwUfVwaai3S2ByZExMSIqAVuBw6SJGAP4K6U7ybg4PT+oPSZdHzPlP8g4PaIWBgR7wIT0vXbRL5lrt4tc2YdWmQ+TR9r0ivyz8P0fOpK88/DfYFRETEzImYBo/jvH8zLzBMgzDqOpa7lki6S9D5wNF9smdspdTk8KGnzlNYXeL8gz+SUtjowOyLqG6V/4Zx0fE7KX+pabSI/eLjWwZxZhyepStIrwDSygOy5lH4D8CGwCXBlwSlfL+h+7Z/SWvwMk3SSpDGSxkyfPn2Jyup15sw6jqWu5RHxk4joD/yZz7sVXgLWTd0QVwL3pHQVu0QT6Ut7zhcsy4Mwr2Zxy5y7Wc06uohYlIaX9AO2l7RFSj8B6AO8ARyRst8HDEjdr4/yeU9Di59hEXFtRAyOiMG9e/deorJ6NqtZx9EatfxW4OuQdb/muyEi4gGgRlIvsl+e/QvO6QdMAWYAPSVVN0qn8Jx0vAdZd2+pa/2XZXkQ5uVns9Z7nTkzSyJiNvA4Bd2jEbEIuIPPn4cfR8TCdPg6YNv0vsXPsGWRn7SV/0FqZpVrqYI5SRsWfDwQeDOlr5XGjSBp+3T9j4EXgA3TzNVOZBMYRka24ek/gUPTtYYD96b3I9Nn0vHHUv6RwJFptut6wIbA80vzPVpicTdrvVvmzDoySb0l9UzvuwJ7AW9JGpjSBBzA58/DtQtOP5Cs1Q7gYWAfSaumiQ/7pLRWtXBRA52qc6RHsplVsOrmMki6DRgK9JI0mWwW1v6SNgYagP8A30nZDwW+K6kemA8cmQKwekmnkj2wqoARETE+nfNj4HZJ/wu8DFyf0q8HbpE0gaxF7kiAiBgv6U7gdaAeOCX9Im4Ti7tZ3TJn1tGtDdyUZs/ngDuBvwNPSupO1n06Fvhuyn+apAPJnlMzgeMBImKmpAvJfuQCXBARX5hk1hpq6xvo7C5Wsw6h2WAuIo4qknx9kTQi4irgqhLHHgAeKJI+kSKzUSNiAXBYiWtdBFxUutStp6bKiwabGUTEOGDrIod2KZH/HOCcEsdGACNar3T/rba+wZMfzDoI1/RmfL5osLtZzax8OJgz6zhc05vRyS1zZlaGahc1eCarWQfhmt6M6vQw9NIkZlZO6ha5Zc6so3BNb0Z1znuzmln5qa1v8O4PZh2Ea3oz8r9sPWbOzMrJQo+ZM+swXNObkW+Z89IkZlZOPAHCrONwTW9GTXV+0WAHc2ZWPmoXNdDZwZxZh+Ca3oyaxdt5uZvVzMpHbb1ns5p1FK7pzcjvAFHnljkzKyN1izwBwqyjcE1vRlV+Nqtb5sysjHjMnFnH4ZreDEnUVIl6L01iZmXEwZxZx+Ga3gI1VTmvM2dmZaXWiwabdRiu6S1QnZPXmTOzsrLQiwabdRiu6S3Qqdotc2ZWXtzNatZxuKa3QHUu571ZzayseDarWcfhmt4CNdVyy5yZlY36RQ00BG6ZM+sgXNNboCaX89IkZlY2atOPTwdzZh2Da3oLVHtpEjMrI/ntB93NatYxuKa3gJcmMbNykg/matwyZ9YhuKa3QHVVzkuTmFnZyHezdnbLnFmH4JreAp2qPAHCzMrH4m5Wt8yZdQiu6S3gpUnMrJx4AoRZx+Ka3gI11TnqGtwyZ2blwRMgzDoW1/QWqMm5m9XMyoe7Wc06Ftf0FsiWJnE3q5mVh8WzWd0yZ9YhuKa3QE1VbvEYFDOzFZ3HzJl1LK7pLdDJ68yZWRnJt8x1djBn1iG4prdAr26dmfbJQhq8pZeZlQG3zJl1LC2q6ZJGSJom6bWCtAsljZP0iqRHJPVJ6ZJ0haQJ6fg2BecMl/R2eg0vSN9W0qvpnCskKaWvJmlUyj9K0qrN3aMtDFh9ZRbWNzD1kwVteRszs1bh2axmHUtLa/qNwLBGab+JiC0jYhBwP3BuSt8P2DC9TgKuhiwwA84DdgC2B87LB2cpz0kF5+XvdTbwj4jYENaE2awAABQTSURBVPhH+lzyHm1lwOorAfCfGZ+15W3MzFqFZ7OadSwtqukR8QQws1HaJwUfVwbyfZAHATdH5lmgp6S1gX2BURExMyJmAf+/vXsPkqss8zj+/fVcMsmQIWQmhJAEEiQgIULQGFnwwk2IrIpbiGC5RQqpQixw0b24UO7qomihuyvLuqsWJWDYVQRvKyoKAUXF4hYwieEe7iGBhASSQJK5dD/7x3l70hl6mEtmprvTv0+lq895znvOeXp6zpt33nPec5YCi9Kytoi4KyICuA74UMm2lqTpJX3i5fYxKmZ1tALw1EY35sys+hVPs3o0q1l9aNydlSV9GTgb2Awcn8LTgedKiq1JsTeKrykTB5gaEesAImKdpH0H2Me63fk8/dmvrYVxjTmeds+cmdUA98yZ1ZfdOtIj4nMRMRP4HnBhCqtc0WHE38ig1pF0nqRlkpZt2LBhgE32L5cTB7ZP4OmN24a9DTOzsVLsmfNoVrP6MFJH+veB09P0GmBmybIZwNoB4jPKxAFeLJ4+Te/rB9jHLiLiqohYEBELpkyZMoyPtdOs9lb3zJlZTfAACLP6MuwjXdKcktkPAo+k6ZuAs9OI06OBzelU6S3AyZL2SQMfTgZuScu2Sjo6jWI9G/hZybaKo14X94mX28eomd3RyjObtvn2JGZW9bp6CjTmRC5X7iSGme1pBnXNnKTrgeOADklryEalnirpUKAAPAOcn4rfDJwKrAa2AecARMQmSV8C7kvlvhgRxUEVnyQbMTse+FV6AVwO3CjpXOBZ4Iw32sdomtXRSldPgbWbtzNjnwmjvTszs2Hr6in4ejmzOjKoxlxEfLRM+Op+ygZwQT/LrgGuKRNfBswrE98InDiUfYyWA4u3J9m4zY05M6tqXfmCR7Ka1REf7YM0u3h7El83Z2ZVrjvvnjmzeuKjfZCmTmyhpcm3JzGz6tfZU/DgB7M64qN9kHI5ceDkVp72jYPNrMp19RR8WxKzOuKjfQhmdfhec2ZW/TwAwqy++GgfglkdrTy7cRt5357EzKpYl6+ZM6srPtqHYHZ7K135Amtf2V7pVMzM+tXt0axmdcVH+xAc2J6NaH3Gp1rNrIp1eQCEWV3x0T4Evbcn8SAIM6tivmbOrL74aB+CqW3jfHsSM6t6nW7MmdUVH+1DIIlZ7a1uzJnVIUktku6VtELSg5IuTfGrU2ylpB9J2ivFx0m6QdJqSfdImlWyrUtS/FFJp4x0rh4AYVZffLQP0ax232vOrE51AidExJHAfGCRpKOBz0TEkRFxBNkzpC9M5c8FXo6Ig4ErgK8CSJoLnAUcDiwCvimpYSQT9TVzZvXFR/sQzepo5blN2317ErM6E5lX02xTekVEbAGQJGA8UKwcTgOWpOkfASemMqcBP4iIzoh4ClgNLBzJXLvzbsyZ1RMf7UM0u2OCb09iVqckNUhaDqwHlkbEPSl+LfAC8GbgG6n4dOA5gIjoATYD7aXxZE2KjRgPgDCrLz7ah6h4e5KnfN2cWd2JiHxEzAdmAAslzUvxc4D9gYeBM1NxldvEG8RfR9J5kpZJWrZhw4ZB5+nGnFl98dE+RMXbkzzj6+bM6lZEvALcQXbNWzGWB24ATk+hNcBMAEmNwN7AptJ4MgNY289+roqIBRGxYMqUKYPOzwMgzOqLj/Yh2nfiOMY3NfDUS75xsFk9kTRF0qQ0PR44CXhU0sEpJuADwCNplZuAxWn6w8BvIiJS/Kw02nU2MAe4d6TyLBSC7nz4mjmzOtJY6QRqjSQObJ/gEa1m9WcasCSNPM0BNwK/BP4gqY3s9OkK4JOp/NXA/0haTdYjdxZARDwo6UbgIaAHuCD16o2IrnwBwD1zZnXEjblhmN3RyqMvbq10GmY2hiJiJXBUmUXH9lN+B3BGP8u+DHx55LLbqbvYmHPPnFnd8NE+DNntSbbRkypNM7Nq0dXjnjmzeuOjfRgO6milOx88uHZLpVMxM9uFT7Oa1R8f7cNw8tz9aGtp5MrbH690KmZmu+jtmfNpVrO64aN9GPae0MT5x72J3zyynvue3lTpdMzMevk0q1n98dE+TOccM5t9J47ja79+hOxuA2ZmldeZGnNN7pkzqxs+2odpfHMDnzpxDvc9/TJ3PDr4O7ObmY2m4mjWce6ZM6sbPtp3w1lvn8mB7RP42i2PUii4d87MKs+nWc3qj4/23dDUkONv33sID6/bws9Xln0aj5nZmPJoVrP646N9N33giP15834T+frSx3pPb5iZVYpHs5rVHx/tuymXE59ddCjPbNzG3/9wBa929lQ6JTOrYz7NalZ/BjzaJV0jab2kVSWxf5X0iKSVkn5a8vDpWZK2S1qeXt8uWedtkv4sabWk/0wPpUbSZElLJT2e3vdJcaVyq9N+3lqyrcWp/OOSFlNhxx+6L5856RB+vmItp175Bx549uVKp2Rmdap4mtWjWc3qx2CO9u8Ci/rElgLzIuII4DHgkpJlT0TE/PQ6vyT+LeA8YE56Fbd5MXB7RMwBbk/zAO8rKXteWh9Jk4EvAO8AFgJfKDYAK0USF500hxs/8RcUIjjj23dx5W2P+3FfZjbmij1zHs1qVj8GPNoj4vfApj6xWyOieD7xbmDGG21D0jSgLSLuiuymbNcBH0qLTwOWpOklfeLXReZuYFLazinA0ojYFBEvkzUs+zY2K2LBrMncfNG7+OCR+3PFbY9x/L/fwVd//Qirnt/se9GZ2ZjwAAiz+tM4Atv4OHBDyfxsSX8CtgD/FBF/AKYDa0rKrEkxgKkRsQ4gItZJ2jfFpwPPlVmnv3hVaGtp4ooz53PK4fvx/Xuf5arfP8m37niC2R2tvHfuVOZN35u509qY3dFKQ06VTtfM9jAeAGFWf3arMSfpc0AP8L0UWgccEBEbJb0N+D9JhwPlWi0DdVX1t86gtyXpPLJTtBxwwAED7G5kLZq3H4vm7cem17q49cEX+MXKdVz7x6fozmeptjTlOGTqRGZOnsD+e7ew/6Tx7D9pPB17jaO9tZl9Wptpa2kkXVpoZjYoHgBhVn+G3ZhLAw/eD5yYTp0SEZ1AZ5q+X9ITwCFkvWelp2JnAMUbs70oaVrqlZsGrE/xNcDMMuusAY7rE7+jXI4RcRVwFcCCBQsqcp5zcmszZy08gLMWHkBXT4EnNrzKQ2u38NC6LTz24lYeXruF2x56sfcRPKUac2LShCYmtjQxsaWRiS2N7DWukdbmRiaMa6C1uZHxzQ2Mb2pgfHMDLU3ZdEtTA+Mac9krTTc35mhu2Pne1JijqUE0N+TcYDTbg7gxZ1Z/htWYk7QI+EfgPRGxrSQ+BdgUEXlJB5ENXngyIjZJ2irpaOAe4GzgG2m1m4DFwOXp/Wcl8Qsl/YBssMPm1OC7BfhKyaCHk9l1AEbVam7Mcdi0Ng6b1sbpJfGIYNNrXazbvIMNr3by8mtdbHqti42vdbF5ezdbd/SwdUf2/tLWbWzr7mFbZ57XunrY0b37gywac6KxQTQ15GhqyNGYy6YbG0RDTjTldk435ERjTuSUrZNTNt+QYg05kUtlGiQk0ZCDnLJ4TvTGc0rzueI8SFlZSQh6y0jZQJPe5RTLZZ9hZ/md5dJwaURan53x4ny28s7uXpWUp0+5vm3eYiN457rFzamkTJ91+vzsX9+Ofn3DeqBtvL782DfOh3NN6GDWKLfZYw5up62lacj7qxfF+102+jIOs7oxYGNO0vVkPWEdktaQjSS9BBgHLE3/cdydRq6+G/iipB4gD5wfEcXBE58kGxk7HvhVekHWiLtR0rnAs8AZKX4zcCqwGtgGnAOQGoZfAu5L5b5Yso+aJIn2vcbRvte4Ia+bLwQ7uvNs786zvSvPju48nT2F3vfOnjyd3QW68gW6ena+d+cLdOejd7qnkE33FAr05IPufJAvFOguBD35LJaPIF/IXj2FoLM7W68YKxSXx875QoHeeCGCQpDiJdPFeCEIsoaBn45m/bn5b97F3P3dmOtPZ75Ac6N73M3qiepllOWCBQti2bJllU7DhqDYqOt9J4jIemsKkTX8CpHFSMuL5bNGYRZL/3rnI3b2CkVx/d75tE7v9M5yUDrfu8Yu86VHU99DK/r0Rb1ueZlDcaB1at1g2hvq0xd50JRWWpoaBrl93R8RC4aTW7UZbB324pYdbNjaybzpe49BVmY2WoZSf43EaFazUSGJhuxcaaVTMasZU9tamNrWUuk0zGwM+QpZMzMzsxrmxpyZmZlZDXNjzszMzKyGuTFnZmZmVsPcmDMzMzOrYW7MmZmZmdUwN+bMzMzMapgbc2ZmZmY1zI05MzMzsxrmxpyZmZlZDaubZ7NK2gA8M4RVOoCXRimdkVYrudZKnuBcR0Ml8jwwIqaM8T5HxRDrsFr5nQDnOhpqJU9wrm9k0PVX3TTmhkrSslp5QHet5ForeYJzHQ21kueeoJZ+1s515NVKnuBcR4pPs5qZmZnVMDfmzMzMzGqYG3P9u6rSCQxBreRaK3mCcx0NtZLnnqCWftbOdeTVSp7gXEeEr5kzMzMzq2HumTMzMzOrYW7M9SFpkaRHJa2WdHGl8ykl6RpJ6yWtKolNlrRU0uPpfZ9K5lgkaaak30p6WNKDki5K8arKV1KLpHslrUh5XprisyXdk/K8QVJzJfMsJalB0p8k/SLNV2Wukp6W9GdJyyUtS7Gq+v73RK7Ddl+t1F8pp5qqw1x/jQ435kpIagD+G3gfMBf4qKS5lc1qF98FFvWJXQzcHhFzgNvTfDXoAf4uIg4DjgYuSD/Lasu3EzghIo4E5gOLJB0NfBW4IuX5MnBuBXPs6yLg4ZL5as71+IiYXzKcv9q+/z2K67ARUyv1F9ReHeb6axS4MberhcDqiHgyIrqAHwCnVTinXhHxe2BTn/BpwJI0vQT40Jgm1Y+IWBcRD6TprWQH73SqLN/IvJpmm9IrgBOAH6V4xfMskjQD+EvgO2leVGmu/aiq738P5DpsBNRK/QW1VYe5/ho9bsztajrwXMn8mhSrZlMjYh1kFRCwb4XzeR1Js4CjgHuownxTt/9yYD2wFHgCeCUielKRavo9+A/gs0AhzbdTvbkGcKuk+yWdl2JV9/3vYVyHjbBqr7+gpuow11+jpLHSCVQZlYl5uO9ukLQX8GPg0xGxJftDrLpERB6YL2kS8FPgsHLFxjar15P0fmB9RNwv6bhiuEzRiueaHBsRayXtCyyV9EilE6oD1fz7UHNqof6C2qjDXH+NLvfM7WoNMLNkfgawtkK5DNaLkqYBpPf1Fc6nl6QmsorwexHxkxSu2nwj4hXgDrJrZCZJKv6xUy2/B8cCH5T0NNnpsxPI/tKtxlyJiLXpfT3ZfzALqeLvfw/hOmyE1Fr9BVVfh7n+GkVuzO3qPmBOGl3TDJwF3FThnAZyE7A4TS8GflbBXHqlayGuBh6OiK+XLKqqfCVNSX/NImk8cBLZ9TG/BT6cilU8T4CIuCQiZkTELLLfzd9ExMeowlwltUqaWJwGTgZWUWXf/x7IddgIqJX6C2qnDnP9Ncoiwq+SF3Aq8BjZNQefq3Q+fXK7HlgHdJP9BX4u2TUHtwOPp/fJlc4z5fpOsu7ylcDy9Dq12vIFjgD+lPJcBXw+xQ8C7gVWAz8ExlX6Z9on7+OAX1RrrimnFen1YPFYqrbvf098uQ4bkTxrov5KudZcHeb6a+RffgKEmZmZWQ3zaVYzMzOzGubGnJmZmVkNc2POzMzMrIa5MWdmZmZWw9yYMzMzM6thbsyZmVldkdQuaXl6vSDp+ZL55kFu41pJhw5Q5gJJHxuhnK+VdKiknKQRfcC7pI9L2q/vvkZyHza6fGsSMzOrW5L+BXg1Iv6tT1xk/0cWyq5YIelpCS9FxKQhrtcQ2WO/yi27E7gwIpaPRI429twzZ2ZmBkg6WNIqSd8GHgCmSbpK0jJJD0r6fEnZOyXNl9Qo6RVJl0taIemu9DxPJF0m6dMl5S+XdK+kRyUdk+Ktkn6c1r0+7Wt+mdzuTPHLgYmpF/G6tGxx2u5ySd9MvXfFvC6TdC+wUNKlku4rfkZlzgTmAzcUeyZL9oWkv5b057TOV1Ks389sleHGnJmZ2U5zgasj4qiIeB64OCIWAEcC75U0t8w6ewO/i4gjgbuAj/ezbUXEQuAfgGLD8FPAC2ndy4GjBsjvYmBrRMyPiLMlzQP+CjgmIuYDjWSPyyrm9UBELIyIu4ArI+LtwFvSskURcQPZEy7OTNvs6k1WmgFcBhyf8jpW0vuH+JltDLgxZ2ZmttMTEXFfyfxHJT1A1lN3GFljr6/tEfGrNH0/MKufbf+kTJl3kj14nogoPj5qKE4C3g4sk7QceA/wprSsi+wh8UUnpl66Fanc4QNs+x1kz1B9KSK6ge8D707LBvuZbQw0VjoBMzOzKvJacULSHOAiYGFEvCLpf4GWMut0lUzn6f//1s4yZbR76SLgmoj4512C2bV12yNdGC9pAvBfwFsj4nlJl1H+s/Tddn8G+5ltDLhnzszMrLw2YCuwRdI04JRR2MedwEcAJL2F8j1/vSKiJ5UtNp5uAz4iqSPF2yUdUGbV8UABeEnSROD0kmVbgYll1rkbOD5ts3j69neD/WA2dtySNjMzK+8B4CFgFfAk8MdR2Mc3gOskrUz7WwVsHmCdq4GVkpal6+YuBW6TlAO6gfOBtaUrRMRGSUvS9p8B7ilZfC3wHUnbgYUl66xJgz7uIOul+3lE/LKkIWlVwrcmMTMzq5DUMGqMiB3ptO6twJxiD5zZYLh1bWZmVjl7AbenRp2AT7ghZ0PlnjkzMzOzGuYBEGZmZmY1zI05MzMzsxrmxpyZmZlZDXNjzszMzKyGuTFnZmZmVsPcmDMzMzOrYf8PGR4jakTt/bsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[10, 5])\n",
    "fig.suptitle(\"Alternating optimization, k=100\")\n",
    "\n",
    "ax[0].plot(train_loss[1::])\n",
    "ax[0].set_title('Training loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "ax[1].plot(val_loss[1::])\n",
    "ax[1].set_title('Validation loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
